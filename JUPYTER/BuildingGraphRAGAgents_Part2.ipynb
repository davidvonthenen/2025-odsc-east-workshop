{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Implementing Reinforcement Learning Using Graph-based RAG"
      ],
      "metadata": {
        "id": "seLlfb-Svq7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will introduce the concept of **reinforcement learning** and explore how it enhances the capabilities of Retrieval-Augmented Generation (RAG) systems. Reinforcement learning involves training models to improve their performance based on feedback from interactions, enabling them to adapt and refine their behavior over time. Here, we will leverage fixed Cypher paths to streamline interactions with a Neo4j knowledge graph, ensuring consistency and efficiency in retrieving data.\n",
        "\n",
        "By combining reinforcement learning with Graph-based RAG, you'll learn how to build adaptive and intelligent agents capable of delivering precise and timely insights.\n",
        "\n",
        "> **IMPORTANT:** You will need to rerun the environment setup since this is a different Jupyter Notebook. This is needed because of the memory limits of Google Colab."
      ],
      "metadata": {
        "id": "GOinQ4EBvpJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Environment Setup"
      ],
      "metadata": {
        "id": "NXg5093ivdKB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpXg5fAtWcLU"
      },
      "source": [
        "To get started, we need to set up two main components of our environment: a Neo4j graph database and a local LLM for question-answering. We'll use **Docker** to run Neo4j, you will need to download an LLM (we'll provide some recommendations) and set up a Python environment for our code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Launch Neo4j with Docker"
      ],
      "metadata": {
        "id": "MgeRzh2I0h0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, spin up a Neo4j instance using Docker. We can use the official Neo4j image and expose the default ports (7474 for HTTP interface, 7687 for Bolt protocol). Below is a **Docker command** that starts a Neo4j instance:"
      ],
      "metadata": {
        "id": "7Bk5oQt40lZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install udocker\n",
        "!udocker --allow-root install\n",
        "\n",
        "!mkdir -p /root/neo4j-inmem\n",
        "!mount -t tmpfs -o size=1g tmpfs /root/neo4j-inmem\n",
        "!mkdir -p /root/neo4j-inmem/data\n",
        "!mkdir -p /root/neo4j-inmem/logs\n",
        "!mkdir -p /root/neo4j-inmem/import\n",
        "!mkdir -p /root/neo4j-inmem/plugins\n",
        "\n",
        "# Run neo4j in docker container\n",
        "!nohup udocker --allow-root run \\\n",
        "  --publish=7474:7474 --publish=7687:7687 \\\n",
        "  --env NEO4J_AUTH=neo4j/neo4jneo4j \\\n",
        "  -v /root/neo4j-inmem/data:/data \\\n",
        "  -v /root/neo4j-inmem/logs:/logs \\\n",
        "  -v /root/neo4j-inmem/import:/var/lib/neo4j/import \\\n",
        "  -v /root/neo4j-inmem/plugins:/plugins \\\n",
        "  -e NEO4JLABS_PLUGINS='[\"apoc\"]' \\\n",
        "  -e NEO4J_apoc_export_file_enabled=true \\\n",
        "  -e NEO4J_apoc_import_file_enabled=true \\\n",
        "  -e NEO4J_dbms_directories_data=/data \\\n",
        "  neo4j:5.26 &\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Setup Docker Complete!\")"
      ],
      "metadata": {
        "id": "hhpN0Su10pIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Python Environment and Dependencies"
      ],
      "metadata": {
        "id": "3RJPdgWe0yP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Neo4j running and the model file ready, set up a Python environment for running the ingestion and querying code. You should have Python 3.10+ available. It's recommended to use a virtual environment or Conda environment for the lab.\n",
        "\n",
        "> **IMPORTANT** This will take ~5-6 minutes to complete on Google Colab."
      ],
      "metadata": {
        "id": "z54iDwkI00Ro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuJ4hCURWc56"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python neo4j==5.28.1 requests==2.32.3 sentence-transformers==4.1.0 ctransformers==0.2.27 spacy==3.8.5\n",
        "\n",
        "!pip install Flask==3.1.0\n",
        "!pip install gdown==5.2.0\n",
        "\n",
        "# download small English model for NER\n",
        "# python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download files from my Public Google Drive\n",
        "print(\"\\n\\n\")\n",
        "print(\"Download Dataset from Google Drive\")\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# backup id: 13GRUxdsUUlUK9uC832Su9Qy2mst9Jznq\n",
        "url = 'https://drive.google.com/uc?id=1f3dqqf9VSnGoVFCP4IozY2AWI39C1UMe'\n",
        "output = 'bbc-mini.zip'\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(output):\n",
        "    print(\"Downloading the zip file...\")\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Zip file already exists, skipping download.\")\n",
        "\n",
        "with zipfile.ZipFile(\"bbc-mini.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Setup Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **IMPORTANT:** Don't move onto the next section until you see a \"Complete!\" in the output for this section."
      ],
      "metadata": {
        "id": "EHJMQO9aAJC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Set Up the Local LLM"
      ],
      "metadata": {
        "id": "4090rCTR3l3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we'll use a 7B parameter model called [neural-chat-7B-v3-3-GGUF](https://huggingface.co/TheBloke/neural-chat-7B-v3-3-GGUF) (a quantized GGUF file). This is the model that will be used in the lab, so for maximum \"it just works\", stick with this model."
      ],
      "metadata": {
        "id": "B_DOmmhs4Sy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/neural-chat-7B-v3-3-GGUF/resolve/main/neural-chat-7b-v3-3.Q4_K_M.gguf\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Download Complete!\")"
      ],
      "metadata": {
        "id": "ZBIPr0hfB_8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **IMPORTANT:** Don't move onto the next section until you see a \"Complete!\" in the output for this section."
      ],
      "metadata": {
        "id": "XZFzTBJL8fjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Ingestion: From Raw Text to a Queryable Graph"
      ],
      "metadata": {
        "id": "hFKPrsoo4VRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the environment ready, we'll proceed to prepare our data (BBC articles) and build the knowledge graph in Neo4j as graph nodes and relationships.\n",
        "\n",
        "Our knowledge source is a collection of BBC news articles in text format which can be found in the zip file [bbc-lite.zip](./workshop/1_llm_cypher/bbc-lite.zip). This zip file ontains a subset of 300 BBC news articles from the 2225 articles in the [BBC Full Text Document Classification](https://bit.ly/4hBKNjp) dataset. After unzipping the archive, the directory structure will look like:\n",
        "\n",
        "```\n",
        "bbc/\n",
        "├── tech/\n",
        "    ├── 001.txt\n",
        "    ├── 002.txt\n",
        "    ├── 003.txt\n",
        "    ├── 004.txt\n",
        "    ├── 005.txt\n",
        "    └── ...\n",
        "```\n",
        "\n",
        "Each file is a news article relating to technology in the world today."
      ],
      "metadata": {
        "id": "Xt_eX35x4Wph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 What We’re Really Building"
      ],
      "metadata": {
        "id": "yblWuoEE4bJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forget vector stores for a moment. We're creating **two node labels** and **one relationship type**-all you need for entity-centric retrieval:\n",
        "\n",
        "| Node Label       | Key Properties           | Purpose                                    |\n",
        "| ---------------- | ------------------------ | ------------------------------------------ |\n",
        "| `:Document`      | `id`, `title`, `content` | Holds the full article text                |\n",
        "| `:Entity`       | `name`                   | Unique named entities (people, orgs, etc.) |\n",
        "| **Relationship** | **Direction**            | **Meaning**                                |\n",
        "| `[:MENTIONS]`    | `(Document) → (Entity)` | \"This article talks about that entity.\"    |\n",
        "\n",
        "No vectors... just raw NER-driven connections that keep the graph clean and demo-ready."
      ],
      "metadata": {
        "id": "peeSIMRb4cbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Ingestion Script"
      ],
      "metadata": {
        "id": "Wh6Vj5jg4e5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will construct the knowledge graph in Neo4j by creating nodes for **documents** and **entities**, and defining relationships among them. Our graph schema will be:\n",
        "\n",
        "* **Document** nodes: each article is a document node with properties like `title` (we'll use filename as title) and `content` (the full text).\n",
        "* **Entity** nodes: significant entities mentioned in the articles (we'll extract these via NER).\n",
        "\n",
        "Relationships:\n",
        "\n",
        "* `(:Document)-[:MENTIONS]->(:Entity)` - links a document to an entity it mentions.\n",
        "\n",
        "We'll use **spaCy** to identify named entities in each article as our \"areas of interest.\" SpaCy's small English model can recognize entities like PERSON, ORG (organization), GPE (location), etc. We'll treat each unique entity text as a Entity node (with an optional property for its type/label)."
      ],
      "metadata": {
        "id": "j4HlZlm14gK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import spacy\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Neo4j connection settings\n",
        "NEO4J_URI = \"bolt://localhost:7687\"\n",
        "NEO4J_USER = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"neo4jneo4j\"\n",
        "\n",
        "# Path to the unzipped BBC dataset folder (with subfolders like 'tech')\n",
        "DATASET_PATH = \"./bbc\"\n",
        "\n",
        "def ingest_bbc_documents_with_ner():\n",
        "    \"\"\"\n",
        "    Ingest BBC documents from the 'technology' subset (or other categories if desired)\n",
        "    and store them in Neo4j with Document and Entity nodes. The code uses spaCy for NER\n",
        "    and links documents to extracted entities using MENTIONS relationships.\n",
        "    \"\"\"\n",
        "    # Load spaCy's small English model for Named Entity Recognition\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Connect to Neo4j\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "    # Perform ingestion in a session\n",
        "    with driver.session() as session:\n",
        "        # Optional: clear old data\n",
        "        print(\"Clearing old data from Neo4j...\")\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
        "        print(\"Old data removed.\\n\")\n",
        "\n",
        "        # Walk through each category folder\n",
        "        for category in os.listdir(DATASET_PATH):\n",
        "            category_path = os.path.join(DATASET_PATH, category)\n",
        "            if not os.path.isdir(category_path):\n",
        "                continue  # Skip non-directories\n",
        "\n",
        "            print(f\"Ingesting documents in category '{category}'...\")\n",
        "            for filename in os.listdir(category_path):\n",
        "                if filename.endswith(\".txt\"):\n",
        "                    filepath = os.path.join(category_path, filename)\n",
        "\n",
        "                    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                        text_content = f.read()\n",
        "\n",
        "                    # Generate a UUID for each document\n",
        "                    doc_uuid = str(uuid.uuid4())\n",
        "\n",
        "                    # Create or MERGE the Document node\n",
        "                    create_doc_query = \"\"\"\n",
        "                    MERGE (d:Document {doc_uuid: $doc_uuid})\n",
        "                    ON CREATE SET\n",
        "                        d.title = $title,\n",
        "                        d.content = $content,\n",
        "                        d.category = $category\n",
        "                    RETURN d\n",
        "                    \"\"\"\n",
        "                    session.run(\n",
        "                        create_doc_query,\n",
        "                        doc_uuid=doc_uuid,\n",
        "                        title=filename,\n",
        "                        content=text_content,\n",
        "                        category=category\n",
        "                    )\n",
        "\n",
        "                    # Named Entity Recognition\n",
        "                    doc_spacy = nlp(text_content)\n",
        "\n",
        "                    # For each recognized entity, MERGE on (name + label)\n",
        "                    # Then create a relationship from the Document to the Entity.\n",
        "                    for ent in doc_spacy.ents:\n",
        "                        # Skip very short or numeric-only entities\n",
        "                        if len(ent.text.strip()) < 3:\n",
        "                            continue\n",
        "\n",
        "                        # Generate a unique ID for new entities\n",
        "                        entity_uuid = str(uuid.uuid4())\n",
        "\n",
        "                        merge_entity_query = \"\"\"\n",
        "                        MERGE (e:Entity {name: $name, label: $label})\n",
        "                        ON CREATE SET e.ent_uuid = $ent_uuid\n",
        "                        RETURN e.ent_uuid as eUUID\n",
        "                        \"\"\"\n",
        "                        record = session.run(\n",
        "                            merge_entity_query,\n",
        "                            name=ent.text.strip(),\n",
        "                            label=ent.label_,\n",
        "                            ent_uuid=entity_uuid\n",
        "                        ).single()\n",
        "\n",
        "                        ent_id = record[\"eUUID\"]\n",
        "\n",
        "                        # Now create relationship by matching on doc_uuid & ent_uuid\n",
        "                        rel_query = \"\"\"\n",
        "                        MATCH (d:Document { doc_uuid: $docId })\n",
        "                        MATCH (e:Entity { ent_uuid: $entId })\n",
        "                        MERGE (d)-[:MENTIONS]->(e)\n",
        "                        \"\"\"\n",
        "                        session.run(\n",
        "                            rel_query,\n",
        "                            docId=doc_uuid,\n",
        "                            entId=ent_id\n",
        "                        )\n",
        "\n",
        "            print(f\"Finished ingesting category '{category}'.\\n\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "# Ingest the data into our RAG pipeline/neo4j\n",
        "ingest_bbc_documents_with_ner()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Ingest Complete!\")"
      ],
      "metadata": {
        "id": "SHNoanbV4i-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have a rich knowledge graph: documents categorized, and connected to the key entities they mention. This graph can answer more complex questions than a pure vector search - for example, we can traverse from categories to entities to documents, etc., to find multi-hop relationships. We'll leverage this graph for querying in the next step.\n",
        "\n",
        "> **IMPORTANT:** Don't move onto the next section until you see a \"Complete!\" in the output for this section."
      ],
      "metadata": {
        "id": "cwfBJSVOBKe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Implementation of Reinforcement Using Graph-based RAG"
      ],
      "metadata": {
        "id": "dRFMyH9M1uSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This reinforcement learning demonstration presents a series of five technology \"facts\" to the user and treats each \"yes\" response as a positive reward signal, storing the approved fact as a Neo4j `:Document` node and linking it to its named entities via `:MENTIONS` relationships that carry a `24-hour expiration timestamp` (short-term memory). Facts the user declines are simply skipped. Immediately afterward, the script runs a RAG-style retrieval: it extracts entities from a question derived from each fact, fetches all `:Document` nodes whose `:MENTIONS` relationships are either still unexpired or have never expired (long-term), concatenates document snippets into a context, and feeds that context plus question to a locally loaded LLaMA model (via llama-cpp-python) to generate an answer. Through this loop of interactive feedback and retrieval-augmented inference, the system \"learns\" which facts to retain and demonstrates how those preferences influence downstream Q&A."
      ],
      "metadata": {
        "id": "7gXYDSUh1u94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Ingesting New Facts in Short-Term Memory"
      ],
      "metadata": {
        "id": "pwPY0lpE1xKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New facts are ingested as Document nodes linked to recognized Entity nodes via MENTIONS relationships stamped with a 24-hour expiration to represent short-term memory.\n",
        "\n",
        "> **IMPORTANT:** You will be asked a series of questions which require a response in the output console below. Hit 'yes' to like 2 of these items and no to the rest."
      ],
      "metadata": {
        "id": "02N0Tx6u1yOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import time\n",
        "import uuid\n",
        "import spacy\n",
        "from neo4j import GraphDatabase\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Neo4j connection settings\n",
        "NEO4J_URI = \"bolt://localhost:7687\"\n",
        "NEO4J_USER = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"neo4jneo4j\"\n",
        "\n",
        "# Path to your local LLaMA model file. Example: \"models/ggml-model-q4_0.bin\"\n",
        "LLAMA_MODEL_PATH = \"./neural-chat-7b-v3-3.Q4_K_M.gguf\"\n",
        "\n",
        "# Five BBC-style technology facts\n",
        "TECH_FACTS = [\n",
        "    \"\"\"\n",
        "    OpenAI has agreed to buy artificial intelligence-assisted coding tool Windsurf for about $3 billion, Bloomberg News reported on Monday, citing people familiar with the matter.\n",
        "    The deal has not yet closed, the report added.\n",
        "\n",
        "    OpenAI declined to comment, while Windsurf did not immediately respond to Reuters' requests for comment.\n",
        "\n",
        "    Windsurf, formerly known as Codeium, had recently been in talks with investors, including General Catalyst and Kleiner Perkins, to raise funding at a $3 billion valuation, according to Bloomberg News.\n",
        "\n",
        "    It was valued at $1.25 billion last August following a $150 million funding round led by venture capital firm General Catalyst. Other investors in the company include Kleiner Perkins and Greenoaks.\n",
        "\n",
        "    The deal, which would be OpenAI's largest acquisition to date, would complement ChatGPT's coding capabilities. The company has been rolling out improvements in coding with the release of each of its newer models, but the competition is heating up.\n",
        "\n",
        "    OpenAI has made several purchases in recent years to boost different segments of its AI products. It bought search and database analytics startup Rockset in a nine-figure stock deal last year, to provide better infrastructure for its enterprise products.\n",
        "\n",
        "    OpenAI's weekly active users surged past 400 million in February, jumping sharply from the 300 million weekly active users in December.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Will the Apple Vision Pro be discontinued? It's certainly starting to look that way. In the last couple of months, numerous reports have emerged suggesting that Apple is either slowing down or completely halting production of its flagship headset.\n",
        "\n",
        "    So, what does that mean for Apple's future in the extended reality market?\n",
        "\n",
        "    Apple has had a rough time with its Vision Pro headset. Despite incredibly hype leading up to the initial release, and the fact that preorders for the device sold out almost instantly, demand for headset has consistently dropped over the last year.\n",
        "\n",
        "    In fact, sales have diminished to the point that rumors have been coming thick and fast. For a while now, industry analysts and tech enthusiasts believe Apple might give up on its XR journey entirely and return its focus to other types of tech (like smartphones).\n",
        "\n",
        "    However, while Apple has failed to achieve its sales targets with the Vision Pro, I don't think they will abandon the XR market entirely. It seems more likely that Apple will view the initial Vision Pro as an experiment, using it to pave the way to new, more popular devices.\n",
        "\n",
        "    Here's what we know about Apple's XR journey right now.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    OpenAI sees itself paying a lower share of revenue to its investor and close partner Microsoft by 2030 than it currently does, The Information reported, citing financial documents.\n",
        "\n",
        "    The news comes after OpenAI this week changed tack on a major restructuring plan to pursue a new plan that would see its for-profit arm becoming a public benefit corporation (PBC) but continue to be controlled by its nonprofit division.\n",
        "\n",
        "    OpenAI currently has an agreement to share 20% of its top line with Microsoft, but the AI company has told investors it expects to share 10% of revenue with its business partners, including Microsoft, by the end of this decade, The Information reported.\n",
        "\n",
        "    Microsoft has invested tens of billions in OpenAI, and the two companies currently have a contract until 2030 that includes revenue sharing from both sides. The deal also gives Microsoft rights to OpenAI IP within its AI products, as well as exclusivity on OpenAI's APIs on Azure.\n",
        "\n",
        "    Microsoft has not yet approved OpenAI's proposed corporate structure, Bloomberg reported on Monday, as the bigger tech company reportedly wants to ensure the new structure protects its multi-billion-dollar investment.\n",
        "\n",
        "    OpenAI and Microsoft did not immediately return requests for comment.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Perplexity, the developer of an AI-powered search engine, is raising a $50 million seed and pre-seed investment fund, CNBC reported. Although the majority of the capital is coming from limited partners, Perplexity is using some of the capital it raised for the company's growth to anchor the fund. Perplexity reportedly raised $500 million at a $9 billion valuation in December.\n",
        "\n",
        "    Perplexity's fund is managed by general partners Kelly Graziadei and Joanna Lee Shevelenko, who in 2018 co-founded an early-stage venture firm, F7 Ventures, according to PitchBook data. F7 has invested in startups like women's health company Midi. It's not clear if Graziadei and Shevelenko will continue to run F7 or if they will focus all their energies on Perplexity's venture fund.\n",
        "\n",
        "    OpenAI also manages an investment fund known as the OpenAI Startup Fund. However, unlike Perplexity, OpenAI claims it does not use its own capital for these investments.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    DeepSeek-R2 is the upcoming AI model from Chinese startup DeepSeek, promising major advancements in multilingual reasoning, code generation, and multimodal capabilities. Scheduled for early 2025, DeepSeek-R2 combines innovative training techniques with efficient resource usage, positioning itself as a serious global competitor to Silicon Valley's top AI technologies.\n",
        "\n",
        "    In the rapidly evolving landscape of artificial intelligence, a new contender is emerging from China that promises to reshape global AI dynamics. DeepSeek, a relatively young AI startup, is making waves with its forthcoming DeepSeek-R2 model—a bold step in China's ambition to lead the global AI race.\n",
        "\n",
        "    As Western tech giants like OpenAI, Anthropic, and Google dominate headlines, DeepSeek's R2 model represents a significant milestone in AI development from the East. With its unique approach to training, multilingual capabilities, and resource efficiency, DeepSeek-R2 isn't just another language model—it's potentially a game-changer for how we think about AI development globally.\n",
        "\n",
        "    What is DeepSeek-R2?\n",
        "    DeepSeek-R2 is a next-generation large language model that builds upon the foundation laid by DeepSeek-R1. According to reports from Reuters, DeepSeek may be accelerating its launch timeline, potentially bringing this advanced AI system to market earlier than the original May 2025 target.\n",
        "\n",
        "    What sets DeepSeek-R2 apart is not just its improved performance metrics but its underlying architecture and training methodology. While R1 established DeepSeek as a serious competitor with strong multilingual and coding capabilities, R2 aims to push these boundaries significantly further while introducing new capabilities that could challenge the dominance of models like GPT-4 and Claude.\n",
        "\n",
        "    DeepSeek-R2 represents China's growing confidence and technical capability in developing frontier AI technologies. The model has been designed from the ground up to be more efficient with computational resources—a critical advantage in the resource-intensive field of large language model development.\n",
        "    \"\"\"\n",
        "]\n",
        "TECH_CHECK =[\n",
        "    \"How much did OpenAI pay for Windsurf?\",\n",
        "    \"What is the status of the Apple Vision Pro?\",\n",
        "    \"What is the revenue share agreement between OpenAI and Microsoft?\",\n",
        "    \"What is Perplexity's new fund?\",\n",
        "    \"What is the significance of DeepSeek-R2?\"\n",
        "]\n",
        "\n",
        "def connect_neo4j():\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "    return driver\n",
        "\n",
        "def setup_neo4j_schema(session):\n",
        "    \"\"\"\n",
        "    Optional: Clear old documents and relationships if desired,\n",
        "    for a fresh run. This will delete all :Document nodes and Entities.\n",
        "    Comment out if you want to preserve prior data.\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "    MATCH (d:Document)\n",
        "    DETACH DELETE d\n",
        "    \"\"\"\n",
        "    session.run(query)\n",
        "\n",
        "    query = \"\"\"\n",
        "    MATCH (e:Entity)\n",
        "    DETACH DELETE e\n",
        "    \"\"\"\n",
        "    session.run(query)\n",
        "\n",
        "def insert_fact_with_expiration(session, fact_text, nlp, expiration_window_seconds=24*60*60):\n",
        "    \"\"\"\n",
        "    Insert the fact as a :Document node. For each recognized entity, create\n",
        "    a :MENTIONS relationship with an expiration time (now + expiration_window_seconds).\n",
        "    \"\"\"\n",
        "    doc_uuid = str(uuid.uuid4())\n",
        "    create_doc_query = \"\"\"\n",
        "    MERGE (d:Document {doc_uuid: $doc_uuid})\n",
        "    ON CREATE SET\n",
        "        d.content = $content,\n",
        "        d.timestamp = timestamp()\n",
        "    RETURN d\n",
        "    \"\"\"\n",
        "    session.run(create_doc_query, doc_uuid=doc_uuid, content=fact_text)\n",
        "\n",
        "    # Named Entity Recognition\n",
        "    doc_spacy = nlp(fact_text)\n",
        "    expiration_time = time.time() + expiration_window_seconds\n",
        "\n",
        "    for ent in doc_spacy.ents:\n",
        "        if len(ent.text.strip()) < 3:\n",
        "            continue\n",
        "\n",
        "        entity_uuid = str(uuid.uuid4())\n",
        "\n",
        "        merge_entity_query = \"\"\"\n",
        "        MERGE (e:Entity {name: $name, label: $label})\n",
        "        ON CREATE SET e.ent_uuid = $ent_uuid\n",
        "        RETURN e\n",
        "        \"\"\"\n",
        "        session.run(\n",
        "            merge_entity_query,\n",
        "            name=ent.text.strip(),\n",
        "            label=ent.label_,\n",
        "            ent_uuid=entity_uuid\n",
        "        )\n",
        "\n",
        "        # Create a short-term mention relationship with an expiration\n",
        "        mentions_query = \"\"\"\n",
        "        MATCH (d:Document {doc_uuid: $docId})\n",
        "        MATCH (e:Entity {ent_uuid: $entId})\n",
        "        MERGE (d)-[m:MENTIONS]->(e)\n",
        "        ON CREATE SET m.expiration = $expiration\n",
        "        \"\"\"\n",
        "        session.run(\n",
        "            mentions_query,\n",
        "            docId=doc_uuid,\n",
        "            entId=entity_uuid,\n",
        "            expiration=expiration_time\n",
        "        )\n",
        "\n",
        "    return doc_uuid\n",
        "\n",
        "def extract_entities_spacy(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text.strip(), ent.label_) for ent in doc.ents if len(ent.text.strip()) >= 3]\n",
        "\n",
        "def fetch_documents_by_entities(session, entity_texts, top_k=5):\n",
        "    \"\"\"\n",
        "    Fetch documents for which there is a :MENTIONS relationship *not expired*\n",
        "    or having no expiration property. That is:\n",
        "      - m.expiration IS NULL (long-term) OR m.expiration > now (unexpired short-term)\n",
        "    Return up to top_k docs sorted by the count of matched entities.\n",
        "    \"\"\"\n",
        "    if not entity_texts:\n",
        "        return []\n",
        "\n",
        "    entity_list_lower = [txt.lower() for txt in entity_texts]\n",
        "    current_time = time.time()\n",
        "\n",
        "    query = \"\"\"\n",
        "    MATCH (d:Document)-[m:MENTIONS]->(e:Entity)\n",
        "    WHERE toLower(e.name) IN $entity_list\n",
        "      AND (m.expiration IS NULL OR m.expiration > $current_time)\n",
        "    WITH d, count(e) AS matchingEntities\n",
        "    ORDER BY matchingEntities DESC\n",
        "    LIMIT $topK\n",
        "    RETURN\n",
        "        d.doc_uuid AS doc_uuid,\n",
        "        d.content AS content,\n",
        "        matchingEntities\n",
        "    \"\"\"\n",
        "    results = session.run(query, entity_list=entity_list_lower, current_time=current_time, topK=top_k)\n",
        "\n",
        "    docs = []\n",
        "    for record in results:\n",
        "        docs.append({\n",
        "            \"doc_uuid\": record[\"doc_uuid\"],\n",
        "            \"content\": record[\"content\"],\n",
        "            \"match_count\": record[\"matchingEntities\"]\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "def generate_answer(llm, question, context):\n",
        "    \"\"\"\n",
        "    Generates an answer using llama-cpp-python.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are given the following context from multiple documents:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Please provide a concise answer.\n",
        "Answer:\n",
        "\"\"\"\n",
        "    output = llm(\n",
        "        prompt,\n",
        "        max_tokens=1024,\n",
        "        temperature=0.2,\n",
        "        stop=[\"Answer:\"]\n",
        "    )\n",
        "    return output[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "def main():\n",
        "    print(\"=== Reinforcement Learning Demo (Single Mechanism for Memory) ===\")\n",
        "\n",
        "    # Load spaCy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Load LLaMA model\n",
        "    print(\"Loading local LLaMA model; please wait...\")\n",
        "    llm = Llama(\n",
        "        model_path=LLAMA_MODEL_PATH,\n",
        "        n_ctx=32768,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    # use_gpu=True,\n",
        "    # n_gpu_layers=-1,      # offload *all* transformer layers to the GPU\n",
        "    # n_threads=2,          # spawn enough CPU threads to feed the GPU\n",
        "    # n_batch=256,          # process 256 tokens at once for throughput\n",
        "    # f16_kv=True,          # store KV cache in half-precision on GPU\n",
        "\n",
        "    driver = connect_neo4j()\n",
        "    with driver.session() as session:\n",
        "        # Optional: Clear existing data\n",
        "        setup_neo4j_schema(session)\n",
        "\n",
        "        # Store or skip each fact\n",
        "        stored_fact_uuids = []\n",
        "        for fact in TECH_FACTS:\n",
        "            print(\"\\nNew Fact Detected:\")\n",
        "            print(f\" -> {fact}\")\n",
        "            decision = input(\"Store this fact for 24 hours? (yes/no): \").strip().lower()\n",
        "\n",
        "            if decision == \"yes\":\n",
        "                doc_uuid = insert_fact_with_expiration(session, fact, nlp)\n",
        "                stored_fact_uuids.append(doc_uuid)\n",
        "                print(f\"Stored with doc_uuid {doc_uuid}\\n\")\n",
        "            else:\n",
        "                print(\"Skipped storing fact.\\n\")\n",
        "\n",
        "        # Now let's do a RAG query test for each fact\n",
        "        for idx, fact in enumerate(TECH_CHECK, start=1):\n",
        "            print(f\"\\n=== RAG Query Test for Fact #{idx} ===\")\n",
        "            question = f\"What do we know related to: \\\"{fact}\\\"?\"\n",
        "            recognized_entities = extract_entities_spacy(question, nlp)\n",
        "            entity_texts = [ent[0] for ent in recognized_entities]\n",
        "\n",
        "            docs = fetch_documents_by_entities(session, entity_texts, top_k=5)\n",
        "            if not docs:\n",
        "                print(\"No documents found for this query.\")\n",
        "                continue\n",
        "\n",
        "            # Build context\n",
        "            combined_context = \"\"\n",
        "            for doc in docs:\n",
        "                snippet = doc[\"content\"][:200].replace(\"\\n\", \" \")\n",
        "                combined_context += f\"\\n---\\nDocUUID: {doc['doc_uuid']}\\nSnippet: {snippet}...\\n\"\n",
        "\n",
        "            final_answer = generate_answer(llm, question, combined_context)\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {final_answer}\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "# run main\n",
        "main()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Reinforcement Learning Complete!\")\n"
      ],
      "metadata": {
        "id": "h3PMRs9F15-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the key considerations you'll want to keep in mind when running, extending, or hardening this RAG-style reinforcement demo:\n",
        "\n",
        "1. **Memory & Expiration Logic**\n",
        "\n",
        "   * **24-hour TTL**: By default `expiration = now + 24h`. You can parameterize `expiration_window_seconds` for shorter or longer short-term memory.\n",
        "   * **Automatic pruning**: Relationships past their `expiration` won't be returned by your RAG query - but they still live in the graph for auditing. If you prefer automatic cleanup, consider Neo4j's `apoc.periodic` jobs or [TTL Procedures](https://neo4j.com/labs/apoc/4.4/temporal/ttl/) to delete or archive expired edges.\n",
        "\n",
        "2. **NER Quality & Granularity**\n",
        "\n",
        "   * **Model choice**: `en_core_web_sm` is lightweight but misses many fine-grained entities. For technical facts you may want `en_core_web_trf` or a custom fine-tuned model.\n",
        "   * **Filtering**: You skip entities shorter than three characters - but you may also want to filter out numeric entities or overly generic terms.\n",
        "\n",
        "3. **Auditability & Data Retention**\n",
        "\n",
        "   * Every `:Document` and `:Entity` persists forever - only `m.expiration` changes. If privacy or storage is a concern, plan a downstream archival process.\n",
        "   * You might also add a `createdBy` or `source` property to track provenance of each fact.\n",
        "\n",
        "Keep these points in mind as you test and evolve the code. They'll help you maintain performance, accuracy, and a clear audit trail as your RAG agent navigates between ephemeral and enduring knowledge."
      ],
      "metadata": {
        "id": "GVKQDDH32mad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Short-Term and Long-Term Memory"
      ],
      "metadata": {
        "id": "yACKldry26LR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short-term memory temporarily records new facts with an expiration timestamp, whereas long-term memory endures by removing that expiration flag to preserve knowledge permanently."
      ],
      "metadata": {
        "id": "M8l4yx4D28Nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Promoting Data from Short-Term to Long-Term Memory Through Reinforcement Learning"
      ],
      "metadata": {
        "id": "9Ol7E01A3Crs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **IMPORTANT:** For this section, type `yes` to promote 1 of these facts into long-term memory and then type `expire` on the second fact."
      ],
      "metadata": {
        "id": "jnZAdVKO_R8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "transfer_and_query_demo.py\n",
        "Unified short-/long-term memory with a single :Document label.\n",
        "Adds a third option (“expire”) to force-expire short-term MENTIONS.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import spacy\n",
        "from neo4j import GraphDatabase\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# ─── Configuration ─────────────────────────────────────────────────────────────\n",
        "NEO4J_URI      = \"bolt://localhost:7687\"\n",
        "NEO4J_USER     = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"neo4jneo4j\"\n",
        "\n",
        "# Path to your GGUF model\n",
        "LLAMA_MODEL_PATH = \"./neural-chat-7b-v3-3.Q4_K_M.gguf\"\n",
        "\n",
        "TECH_CHECK = [\n",
        "    \"How much did OpenAI pay for Windsurf?\",\n",
        "    \"What is the status of the Apple Vision Pro?\",\n",
        "    \"What is the revenue share agreement between OpenAI and Microsoft?\",\n",
        "    \"What is Perplexity's new fund?\",\n",
        "    \"What is the significance of DeepSeek-R2?\"\n",
        "]\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "# ─── Neo4j Helpers ─────────────────────────────────────────────────────────────\n",
        "def connect_neo4j():\n",
        "    return GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "\n",
        "def find_documents_with_unexpired_mentions(session):\n",
        "    \"\"\"\n",
        "    Return documents whose :MENTIONS relationships still have a future expiration.\n",
        "    \"\"\"\n",
        "    now = time.time()\n",
        "    query = \"\"\"\n",
        "    MATCH (d:Document)-[m:MENTIONS]->(e:Entity)\n",
        "    WHERE m.expiration IS NOT NULL AND m.expiration > $now\n",
        "    WITH d, collect(DISTINCT e.name) AS entities\n",
        "    RETURN d.doc_uuid AS uuid, d.content AS content, entities\n",
        "    ORDER BY d.timestamp ASC\n",
        "    \"\"\"\n",
        "    return [\n",
        "        {\"uuid\": r[\"uuid\"], \"content\": r[\"content\"], \"entities\": r[\"entities\"]}\n",
        "        for r in session.run(query, now=now)\n",
        "    ]\n",
        "\n",
        "\n",
        "def promote_to_long_term(session, doc_uuid):\n",
        "    \"\"\"Remove expiration ⇒ promote to long-term.\"\"\"\n",
        "    session.run(\n",
        "        \"\"\"\n",
        "        MATCH (d:Document {doc_uuid:$uuid})-[m:MENTIONS]->()\n",
        "        REMOVE m.expiration\n",
        "        \"\"\",\n",
        "        uuid=doc_uuid,\n",
        "    )\n",
        "    print(f\"Promoted {doc_uuid} to long-term (expiration removed).\")\n",
        "\n",
        "\n",
        "def force_expire(session, doc_uuid, seconds_ago=2 * 24 * 60 * 60):\n",
        "    \"\"\"Force-expire by setting expiration to NOW - 2 days (default).\"\"\"\n",
        "    past = time.time() - seconds_ago\n",
        "    session.run(\n",
        "        \"\"\"\n",
        "        MATCH (d:Document {doc_uuid:$uuid})-[m:MENTIONS]->()\n",
        "        SET m.expiration = $past\n",
        "        \"\"\",\n",
        "        uuid=doc_uuid,\n",
        "        past=past,\n",
        "    )\n",
        "    print(f\"Forced expiration on {doc_uuid} (set to 2 days ago).\")\n",
        "\n",
        "\n",
        "def fetch_documents_by_entities(session, entity_texts, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieve docs where MENTIONS are unexpired or permanent.\n",
        "    \"\"\"\n",
        "    if not entity_texts:\n",
        "        return []\n",
        "\n",
        "    now = time.time()\n",
        "    entity_list = [t.lower() for t in entity_texts]\n",
        "\n",
        "    query = \"\"\"\n",
        "    MATCH (d:Document)-[m:MENTIONS]->(e:Entity)\n",
        "    WHERE toLower(e.name) IN $entity_list\n",
        "      AND (m.expiration IS NULL OR m.expiration > $now)\n",
        "    WITH d, count(e) AS matches\n",
        "    ORDER BY matches DESC\n",
        "    LIMIT $topK\n",
        "    RETURN d.doc_uuid AS uuid, d.content AS content, matches\n",
        "    \"\"\"\n",
        "\n",
        "    return [\n",
        "        {\"uuid\": r[\"uuid\"], \"content\": r[\"content\"], \"matches\": r[\"matches\"]}\n",
        "        for r in session.run(query, entity_list=entity_list, now=now, topK=top_k)\n",
        "    ]\n",
        "\n",
        "\n",
        "# ─── LLM / NLP Helpers ─────────────────────────────────────────────────────────\n",
        "def extract_entities(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    return [ent.text.strip() for ent in doc.ents if len(ent.text.strip()) >= 3]\n",
        "\n",
        "\n",
        "def generate_answer(llm, question, context):\n",
        "    prompt = f\"\"\"You are given the following context from multiple documents:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    res = llm(prompt, max_tokens=2048, temperature=0.2, stop=[\"Answer:\"])\n",
        "    return res[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "\n",
        "# ─── Main Workflow ─────────────────────────────────────────────────────────────\n",
        "def main():\n",
        "    print(\"=== Transfer & Query Demo (single memory with 'expire' option) ===\")\n",
        "\n",
        "    # Load NLP & LLM\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"Loading local LLaMA model...\")\n",
        "    llm = Llama(\n",
        "        model_path=LLAMA_MODEL_PATH,\n",
        "        n_ctx=32768,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    # use_gpu=True,\n",
        "    # n_gpu_layers=-1,      # offload *all* transformer layers to the GPU\n",
        "    # n_threads=2,          # spawn enough CPU threads to feed the GPU\n",
        "    # n_batch=256,          # process 256 tokens at once for throughput\n",
        "    # f16_kv=True,          # store KV cache in half-precision on GPU\n",
        "\n",
        "    driver = connect_neo4j()\n",
        "    with driver.session() as session:\n",
        "        # 1. Review unexpired short-term docs\n",
        "        docs = find_documents_with_unexpired_mentions(session)\n",
        "        if not docs:\n",
        "            print(\"No unexpired short-term documents found.\")\n",
        "        else:\n",
        "            for d in docs:\n",
        "                print(f\"\\nDocUUID: {d['uuid']}\")\n",
        "                print(f\"Content: {d['content']}\")\n",
        "                print(f\"Entities: {d['entities']}\")\n",
        "                choice = input(\n",
        "                    \"Remove expiration (promote to long-term)? \"\n",
        "                    \"(yes/no/expire): \"\n",
        "                ).strip().lower()\n",
        "\n",
        "                if choice == \"yes\":\n",
        "                    promote_to_long_term(session, d[\"uuid\"])\n",
        "                elif choice == \"expire\":\n",
        "                    force_expire(session, d[\"uuid\"])\n",
        "                else:\n",
        "                    print(\"Leaving document unchanged.\")\n",
        "\n",
        "        # 2. Run RAG queries for the TECH_CHECK questions\n",
        "        for idx, fact in enumerate(TECH_CHECK, start=1):\n",
        "            print(f\"\\n=== RAG Query Test for Fact #{idx} ===\")\n",
        "            question = f\"What do we know related to: \\\"{fact}\\\"?\"\n",
        "\n",
        "            entity_texts = extract_entities(question, nlp)\n",
        "            docs = fetch_documents_by_entities(session, entity_texts, top_k=5)\n",
        "            if not docs:\n",
        "                print(\"No documents found for this query.\")\n",
        "                continue\n",
        "\n",
        "            # Build context\n",
        "            combined_context = \"\"\n",
        "            for doc in docs:\n",
        "                snippet = doc[\"content\"][:200].replace(\"\\n\", \" \")\n",
        "                combined_context += (\n",
        "                    f\"\\n---\\nDocUUID: {doc['uuid']}\\nSnippet: {snippet}...\\n\"\n",
        "                )\n",
        "\n",
        "            answer = generate_answer(llm, question, combined_context)\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "# run main\n",
        "main()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Positive and Negative Reinforcement Complete!\")"
      ],
      "metadata": {
        "id": "QhqqzrKB3E3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see only one of these new facts with documents found in the RAG Agent based on the selections you provided! The other will have been \"forgotten\".\n",
        "\n",
        "Here's a curated checklist of critical considerations to keep this reinforcement-learning + RAG pipeline robust, performant, and maintainable:\n",
        "\n",
        "1. **Expiration & Time Synchronization**\n",
        "\n",
        "   * **Clock Skew**: All expiration logic relies on the host's system time. If your Neo4j server and application server clocks drift, short-term facts could expire prematurely (or hang around indefinitely). Consider NTP synchronization.\n",
        "   * **Expiration Granularity**: Using `time.time()` (float seconds) vs. Neo4j's `timestamp()` (milliseconds) demands careful unit conversions - mismatches may filter out docs incorrectly.\n",
        "\n",
        "2. **NER Coverage & Noise**\n",
        "\n",
        "   * **Short Texts**: Very brief facts may yield no entities, so your RAG queries return empty context. You might want a fallback (e.g. keyword search) for \"no-entity\" cases.\n",
        "   * **Entity Normalization**: spaCy may extract overlapping or partial entities (\"OpenAI\" vs. \"OpenAI Inc.\"). Merging on `name` alone can fragment your graph. Consider lowercasing, stripping punctuation, or using a dedicated alias table.\n",
        "\n",
        "3. **Session & Transaction Management**\n",
        "\n",
        "   * **Batching vs. Per-Relationship Commits**: Each MERGE/CREATE currently runs in its own transaction. For bulk ingest or high throughput, wrap multiple operations in a single transaction to reduce overhead.\n",
        "   * **Error Handling**: Uncaught exceptions (e.g., network blips, write conflicts) will crash the script. Consider try/except around sessions, with retries or circuit-breakers.\n",
        "\n",
        "4. **Auditability & Data Retention**\n",
        "\n",
        "   * **Historical Facts**: Once the 24h expiration lapses, mentions vanish from RAG queries. If you need to debug or audit, consider logging or archiving expired relationships rather than letting them slip away.\n",
        "   * **Promotions**: If you ever remove expiration (promote to long-term), record that event (e.g., with a timestamp or provenance field) so you know why a document persisted.\n",
        "\n",
        "5. **Edge Cases & Emergency Overrides**\n",
        "\n",
        "    * **\"No Entities Found\"**: If a user question yields no entities, handle gracefully - perhaps by returning a canned message or falling back to \"search all unexpired docs.\"\n",
        "    * **Forced Expiration**: Be mindful that setting expiration two days in the past effectively hides docs forever. If that happens by accident, you'll need a manual override to restore them.\n",
        "\n",
        "By keeping these points in view, you'll avoid the classic \"it worked on my laptop\" pitfalls and ensure your unified memory + RAG system remains reliable, performant, and secure in production."
      ],
      "metadata": {
        "id": "UNp9q5Iz46Mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2: Understanding How Reinforcement Learning Works: Vector vs Graph"
      ],
      "metadata": {
        "id": "TNEkKsJv4_OC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine teaching an AI system what's \"important\" and what's \"forgettable,\" not unlike deciding which jokes survive at your next stand-up set. In RAG (Retrieval-Augmented Generation) pipelines, \"reinforcement learning\" isn't just about reward functions - it can also describe how our system **reinforces** or **expires** knowledge. Two dominant paradigms emerge:\n",
        "\n",
        "1. **Vector-based memory** (think embedding indexes in FAISS or Pinecone).\n",
        "2. **Graph-based memory** (our Neo4j + spaCy solution with expiring `MENTIONS` edges).\n",
        "\n",
        "Let's dive into how each handles the \"yes/no\" decision process, and why the graph approach offers fine-grained control over your AI's short-term and long-term facts.\n",
        "\n",
        "#### Vector-Based Memory: The Quick and Dirty Cache\n",
        "\n",
        "* **Mechanism**: Every new fact is converted into a fixed-length embedding, then appended to a vector index.\n",
        "* **Reinforcement Analogy**: Accept a fact? Push its vector. Reject it? Don't.\n",
        "* **Expiry**: Often you must delete vectors manually or retrain your index to \"forget.\" There's no native timestamp on individual embeddings - you're juggling IDs, rebuilds, and hoping you tracked them properly.\n",
        "* **Pros**: Blazing-fast similarity searches; turnkey solutions in FAISS, Annoy, Pinecone.\n",
        "* **Cons**: Coarse control - deleting or promoting facts is a blunt operation. No built-in audit trail.\n",
        "\n",
        "> **Real-world quip**: It's like tossing important receipts into a shredder when they expire - you can do it, but you have to remember which bin you used.\n",
        "\n",
        "#### Graph-Based Memory: Precision and Auditability\n",
        "\n",
        "Our Neo4j + spaCy + llama-cpp setup brings structure to the party:\n",
        "\n",
        "1. **Document Nodes** (`:Document`): Every stored fact - be it short-term or long-term - lives here.\n",
        "\n",
        "2. **Entity Nodes** (`:Entity`): Extracted by spaCy NER, these anchor the \"who,\" \"what,\" and \"where.\"\n",
        "\n",
        "3. **MENTIONS Relationships** (`:Document`→`Entity`):\n",
        "\n",
        "   * **`expiration` property** (Unix timestamp):\n",
        "\n",
        "     * **Short-term**: `expiration = now + 24h`.\n",
        "     * **Long-term**: `expiration = NULL`.\n",
        "\n",
        "   * **Promote to Long-Term**: simply **remove** the `expiration` field.\n",
        "   * **Force-Expire**: set `expiration` to two days ago, effectively hiding it from any future queries.\n",
        "\n",
        "4. **RAG Queries**:\n",
        "\n",
        "   * Match only `MENTIONS` where `expiration IS NULL` **or** `expiration > now`.\n",
        "   * Build context snippets and feed them to LLaMA via llama-cpp.\n",
        "\n",
        "#### Why This Matters\n",
        "\n",
        "* **Granular Control**: Pin-point which facts you want to expire or promote without touching the nodes themselves.\n",
        "* **Audit Trail**: Documents and entities remain intact - perfect for compliance or later analysis.\n",
        "* **Unified Retrieval**: Single Cypher query handles both short- and long-term facts seamlessly."
      ],
      "metadata": {
        "id": "eGo0HMEi4DR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop End!"
      ],
      "metadata": {
        "id": "_BF-oZad5pGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we understand how Reinforcement Learning works in relation to Graph-based RAG implementations, you are finished with this workshop."
      ],
      "metadata": {
        "id": "k_JC58If5rym"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}