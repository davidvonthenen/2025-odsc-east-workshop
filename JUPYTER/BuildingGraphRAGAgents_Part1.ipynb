{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Building a Graph-Based RAG Agent with Neo4j and LLM-generated Cyphers"
      ],
      "metadata": {
        "id": "seLlfb-Svq7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will build a **Retrieval-Augmented Generation (RAG)** pipeline that uses a **knowledge graph** (Neo4j) instead of a traditional vector database for retrieval. We'll ingest a corpus of BBC news articles into Neo4j as a graph of connected entities, then use a local large language model (LLM) to translate natural language questions into Cypher queries with `LangChain`'s `GraphCypherQAChain`. The LLM will execute those queries on the graph and generate answers based on the results.\n",
        "\n",
        "By the end of this lab, you will have a working environment with Neo4j and a local LLM, a Neo4j graph populated with documents, categories, and key entities (with relationships like `BELONGS_TO` and `MENTIONS`), and examples of querying the graph using natural language questions."
      ],
      "metadata": {
        "id": "GOinQ4EBvpJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Environment Setup"
      ],
      "metadata": {
        "id": "NXg5093ivdKB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpXg5fAtWcLU"
      },
      "source": [
        "To get started, we need to set up two main components of our environment: a Neo4j graph database and a local LLM for question-answering. We'll use **Docker** to run Neo4j, you will need to download an LLM (we'll provide some recommendations) and set up a Python environment for our code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Launch Neo4j with Docker"
      ],
      "metadata": {
        "id": "MgeRzh2I0h0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, spin up a Neo4j instance using Docker. We can use the official Neo4j image and expose the default ports (7474 for HTTP interface, 7687 for Bolt protocol). Below is a **Docker command** that starts a Neo4j instance:"
      ],
      "metadata": {
        "id": "7Bk5oQt40lZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install udocker\n",
        "!udocker --allow-root install\n",
        "\n",
        "!mkdir -p /root/neo4j-inmem\n",
        "!mount -t tmpfs -o size=1g tmpfs /root/neo4j-inmem\n",
        "!mkdir -p /root/neo4j-inmem/data\n",
        "!mkdir -p /root/neo4j-inmem/logs\n",
        "!mkdir -p /root/neo4j-inmem/import\n",
        "!mkdir -p /root/neo4j-inmem/plugins\n",
        "\n",
        "# Run neo4j in docker container\n",
        "!nohup udocker --allow-root run \\\n",
        "  --publish=7474:7474 --publish=7687:7687 \\\n",
        "  --env NEO4J_AUTH=neo4j/neo4jneo4j \\\n",
        "  -v /root/neo4j-inmem/data:/data \\\n",
        "  -v /root/neo4j-inmem/logs:/logs \\\n",
        "  -v /root/neo4j-inmem/import:/var/lib/neo4j/import \\\n",
        "  -v /root/neo4j-inmem/plugins:/plugins \\\n",
        "  -e NEO4JLABS_PLUGINS='[\"apoc\"]' \\\n",
        "  -e NEO4J_apoc_export_file_enabled=true \\\n",
        "  -e NEO4J_apoc_import_file_enabled=true \\\n",
        "  -e NEO4J_dbms_directories_data=/data \\\n",
        "  neo4j:5.26 &\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Setup Docker Complete!\")"
      ],
      "metadata": {
        "id": "hhpN0Su10pIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Python Environment and Dependencies"
      ],
      "metadata": {
        "id": "3RJPdgWe0yP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Neo4j running and the model file ready, set up a Python environment for running the ingestion and querying code. You should have Python 3.10+ available. It's recommended to use a virtual environment or Conda environment for the lab.\n",
        "\n",
        "> **IMPORTANT** This will take ~5-6 minutes to complete on Google Colab."
      ],
      "metadata": {
        "id": "z54iDwkI00Ro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuJ4hCURWc56"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python neo4j==5.28.1 requests==2.32.3 sentence-transformers==4.1.0 ctransformers==0.2.27 spacy==3.8.5 langchain==0.3.25 langchain-neo4j==0.4.0 langchain-community==0.3.23\n",
        "\n",
        "!pip install Flask==3.1.0\n",
        "!pip install gdown==5.2.0\n",
        "\n",
        "# download small English model for NER\n",
        "# python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download files from my Public Google Drive\n",
        "print(\"\\n\\n\")\n",
        "print(\"Download Dataset from Google Drive\")\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# backup id: 13GRUxdsUUlUK9uC832Su9Qy2mst9Jznq\n",
        "url = 'https://drive.google.com/uc?id=1f3dqqf9VSnGoVFCP4IozY2AWI39C1UMe'\n",
        "output = 'bbc-mini.zip'\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(output):\n",
        "    print(\"Downloading the zip file...\")\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Zip file already exists, skipping download.\")\n",
        "\n",
        "with zipfile.ZipFile(\"bbc-mini.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Setup Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **IMPORTANT:** Don't move onto the next section until you see a \"Complete!\" in the output for this section."
      ],
      "metadata": {
        "id": "EHJMQO9aAJC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Set Up the Local LLM"
      ],
      "metadata": {
        "id": "4090rCTR3l3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we'll use a 7B parameter model called [neural-chat-7B-v3-3-GGUF](https://huggingface.co/TheBloke/neural-chat-7B-v3-3-GGUF) (a quantized GGUF file). This is the model that will be used in the lab, so for maximum \"it just works\", stick with this model."
      ],
      "metadata": {
        "id": "B_DOmmhs4Sy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/neural-chat-7B-v3-3-GGUF/resolve/main/neural-chat-7b-v3-3.Q4_K_M.gguf\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Download Complete!\")"
      ],
      "metadata": {
        "id": "ZBIPr0hfB_8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **IMPORTANT:** Don't move onto the next section until you see a \"Complete!\" in the output for this section."
      ],
      "metadata": {
        "id": "XZFzTBJL8fjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Ingestion: From Raw Text to a Queryable Graph"
      ],
      "metadata": {
        "id": "hFKPrsoo4VRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the environment ready, we'll proceed to prepare our data (BBC articles) and build the knowledge graph in Neo4j as graph nodes and relationships.\n",
        "\n",
        "Our knowledge source is a collection of BBC news articles in text format which can be found in the zip file [bbc-lite.zip](./workshop/1_llm_cypher/bbc-lite.zip). This zip file ontains a subset of 300 BBC news articles from the 2225 articles in the [BBC Full Text Document Classification](https://bit.ly/4hBKNjp) dataset. After unzipping the archive, the directory structure will look like:\n",
        "\n",
        "```\n",
        "bbc/\n",
        "├── tech/\n",
        "    ├── 001.txt\n",
        "    ├── 002.txt\n",
        "    ├── 003.txt\n",
        "    ├── 004.txt\n",
        "    ├── 005.txt\n",
        "    └── ...\n",
        "```\n",
        "\n",
        "Each file is a news article relating to technology in the world today."
      ],
      "metadata": {
        "id": "Xt_eX35x4Wph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 What We’re Really Building"
      ],
      "metadata": {
        "id": "yblWuoEE4bJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forget vector stores for a moment. We're creating **two node labels** and **one relationship type**-all you need for entity-centric retrieval:\n",
        "\n",
        "| Node Label       | Key Properties           | Purpose                                    |\n",
        "| ---------------- | ------------------------ | ------------------------------------------ |\n",
        "| `:Document`      | `id`, `title`, `content` | Holds the full article text                |\n",
        "| `:Entity`       | `name`                   | Unique named entities (people, orgs, etc.) |\n",
        "| **Relationship** | **Direction**            | **Meaning**                                |\n",
        "| `[:MENTIONS]`    | `(Document) → (Entity)` | \"This article talks about that entity.\"    |\n",
        "\n",
        "No vectors... just raw NER-driven connections that keep the graph clean and demo-ready."
      ],
      "metadata": {
        "id": "peeSIMRb4cbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Ingestion Script"
      ],
      "metadata": {
        "id": "Wh6Vj5jg4e5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will construct the knowledge graph in Neo4j by creating nodes for **documents** and **entities**, and defining relationships among them. Our graph schema will be:\n",
        "\n",
        "* **Document** nodes: each article is a document node with properties like `title` (we'll use filename as title) and `content` (the full text).\n",
        "* **Entity** nodes: significant entities mentioned in the articles (we'll extract these via NER).\n",
        "\n",
        "Relationships:\n",
        "\n",
        "* `(:Document)-[:MENTIONS]->(:Entity)` - links a document to an entity it mentions.\n",
        "\n",
        "We'll use **spaCy** to identify named entities in each article as our \"areas of interest.\" SpaCy's small English model can recognize entities like PERSON, ORG (organization), GPE (location), etc. We'll treat each unique entity text as a Entity node (with an optional property for its type/label)."
      ],
      "metadata": {
        "id": "j4HlZlm14gK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import spacy\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Neo4j connection settings\n",
        "NEO4J_URI = \"bolt://localhost:7687\"\n",
        "NEO4J_USER = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"neo4jneo4j\"\n",
        "\n",
        "# Path to the unzipped BBC dataset folder (with subfolders like 'tech')\n",
        "DATASET_PATH = \"./bbc\"\n",
        "\n",
        "def ingest_bbc_documents_with_ner():\n",
        "    \"\"\"\n",
        "    Ingest BBC documents from the 'technology' subset (or other categories if desired)\n",
        "    and store them in Neo4j with Document and Entity nodes. The code uses spaCy for NER\n",
        "    and links documents to extracted entities using MENTIONS relationships.\n",
        "    \"\"\"\n",
        "    # Load spaCy's small English model for Named Entity Recognition\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Connect to Neo4j\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "    # Perform ingestion in a session\n",
        "    with driver.session() as session:\n",
        "        # Optional: clear old data\n",
        "        print(\"Clearing old data from Neo4j...\")\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
        "        print(\"Old data removed.\\n\")\n",
        "\n",
        "        # Walk through each category folder\n",
        "        for category in os.listdir(DATASET_PATH):\n",
        "            category_path = os.path.join(DATASET_PATH, category)\n",
        "            if not os.path.isdir(category_path):\n",
        "                continue  # Skip non-directories\n",
        "\n",
        "            print(f\"Ingesting documents in category '{category}'...\")\n",
        "            for filename in os.listdir(category_path):\n",
        "                if filename.endswith(\".txt\"):\n",
        "                    filepath = os.path.join(category_path, filename)\n",
        "\n",
        "                    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                        text_content = f.read()\n",
        "\n",
        "                    # Generate a UUID for each document\n",
        "                    doc_uuid = str(uuid.uuid4())\n",
        "\n",
        "                    # Create or MERGE the Document node\n",
        "                    create_doc_query = \"\"\"\n",
        "                    MERGE (d:Document {doc_uuid: $doc_uuid})\n",
        "                    ON CREATE SET\n",
        "                        d.title = $title,\n",
        "                        d.content = $content,\n",
        "                        d.category = $category\n",
        "                    RETURN d\n",
        "                    \"\"\"\n",
        "                    session.run(\n",
        "                        create_doc_query,\n",
        "                        doc_uuid=doc_uuid,\n",
        "                        title=filename,\n",
        "                        content=text_content,\n",
        "                        category=category\n",
        "                    )\n",
        "\n",
        "                    # Named Entity Recognition\n",
        "                    doc_spacy = nlp(text_content)\n",
        "\n",
        "                    # For each recognized entity, MERGE on (name + label)\n",
        "                    # Then create a relationship from the Document to the Entity.\n",
        "                    for ent in doc_spacy.ents:\n",
        "                        # Skip very short or numeric-only entities\n",
        "                        if len(ent.text.strip()) < 3:\n",
        "                            continue\n",
        "\n",
        "                        # Generate a unique ID for new entities\n",
        "                        entity_uuid = str(uuid.uuid4())\n",
        "\n",
        "                        merge_entity_query = \"\"\"\n",
        "                        MERGE (e:Entity {name: $name, label: $label})\n",
        "                        ON CREATE SET e.ent_uuid = $ent_uuid\n",
        "                        RETURN e.ent_uuid as eUUID\n",
        "                        \"\"\"\n",
        "                        record = session.run(\n",
        "                            merge_entity_query,\n",
        "                            name=ent.text.strip(),\n",
        "                            label=ent.label_,\n",
        "                            ent_uuid=entity_uuid\n",
        "                        ).single()\n",
        "\n",
        "                        ent_id = record[\"eUUID\"]\n",
        "\n",
        "                        # Now create relationship by matching on doc_uuid & ent_uuid\n",
        "                        rel_query = \"\"\"\n",
        "                        MATCH (d:Document { doc_uuid: $docId })\n",
        "                        MATCH (e:Entity { ent_uuid: $entId })\n",
        "                        MERGE (d)-[:MENTIONS]->(e)\n",
        "                        \"\"\"\n",
        "                        session.run(\n",
        "                            rel_query,\n",
        "                            docId=doc_uuid,\n",
        "                            entId=ent_id\n",
        "                        )\n",
        "\n",
        "            print(f\"Finished ingesting category '{category}'.\\n\")\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "# Ingest the data into our RAG pipeline/neo4j\n",
        "ingest_bbc_documents_with_ner()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Ingest Complete!\")"
      ],
      "metadata": {
        "id": "SHNoanbV4i-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have a rich knowledge graph: documents categorized, and connected to the key entities they mention. This graph can answer more complex questions than a pure vector search - for example, we can traverse from categories to entities to documents, etc., to find multi-hop relationships. We'll leverage this graph for querying in the next step.\n",
        "\n",
        "> **IMPORTANT:** Don't move onto the next section until you see a \"Complete!\" in the output for this section."
      ],
      "metadata": {
        "id": "cwfBJSVOBKe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Querying with LangChain’s GraphCypherQAChain"
      ],
      "metadata": {
        "id": "5Y7VceVeBN6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now comes the exciting part: using an LLM (the one we set up in Step 1) to query the Neo4j graph with natural language. LangChain provides a chain specifically for this purpose called **GraphCypherQAChain**. This chain integrates an LLM with a graph by having the **LLM generate Cypher queries** in response to user questions, retrieving data from the graph, and then formulating a final answer. In other words, the LLM acts as a translator from English to Cypher and then uses the query results to compose an answer.\n",
        "\n",
        "We will configure LangChain's GraphCypherQAChain to use:\n",
        "\n",
        "* Our **local LLM** as the language model that will do the reasoning and query generation.\n",
        "* A **Neo4j graph** connection (pointing to our populated database) for executing the Cypher queries."
      ],
      "metadata": {
        "id": "h6HTTgILBPhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Hook Up LangChain to Neo4j and the Local LLM"
      ],
      "metadata": {
        "id": "s2DIAJY8BRvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will perform the RAG Query using our Graph-based implementation on Neo4j. Based on the data ingested, we will ask the RAG Agent the following questions:\n",
        "\n",
        "- What are the top 5 most mentioned entities in these articles?\n",
        "\n",
        "> **IMPORTANT:** Running the script below will take approximately 4-5 minutes."
      ],
      "metadata": {
        "id": "E3cG7kZRB2bM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qlWSQXEyBT87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# ────────────────────────────── Neo4j Settings ───────────────────────────────\n",
        "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
        "NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
        "NEO4J_PASS = os.getenv(\"NEO4J_PASS\", \"neo4jneo4j\")\n",
        "\n",
        "# ───────────────────────────── llama-cpp Settings ────────────────────────────\n",
        "MODEL_PATH = \"./neural-chat-7b-v3-3.Q4_K_M.gguf\"  # Adjust to your local model\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Prompt template for generating Cypher queries only. We are instructing the LLM\n",
        "# to return a single valid Cypher query.\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "CYTHER_ONLY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"schema\", \"query\"],\n",
        "    template=(\n",
        "        \"You are an expert in Neo4j Cypher.\\n\"\n",
        "        \"Graph schema:\\n{schema}\\n\\n\"\n",
        "        \"Given a natural-language question, return ONE valid Cypher query \"\n",
        "        \"that answers it.\\n\"\n",
        "        \"Output **only** the Cypher query—no explanation, no labels, no \"\n",
        "        \"markdown fences.\\n\\n\"\n",
        "        \"{query}\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "def wait_for_neo4j(uri: str, user: str, pwd: str, tries: int = 10, delay: int = 3):\n",
        "    \"\"\"\n",
        "    Ping the DB until it responds to 'RETURN 1'. This ensures the DB is up\n",
        "    before the script attempts to run queries.\n",
        "    \"\"\"\n",
        "    for i in range(1, tries + 1):\n",
        "        try:\n",
        "            graph = Neo4jGraph(url=uri, username=user, password=pwd)\n",
        "            graph.query(\"RETURN 1\")\n",
        "            print(f\"✓ Neo4j is ready on {uri}\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            print(f\"[{i}/{tries}] Neo4j not reachable ({e}); retrying in {delay}s\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "    sys.exit(\"Neo4j never came online. Exiting.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1) Ensure the Neo4j database is reachable\n",
        "    wait_for_neo4j(NEO4J_URI, NEO4J_USER, NEO4J_PASS)\n",
        "\n",
        "    # 2) Load the graph (this uses the Neo4jGraph class from langchain_community)\n",
        "    graph = Neo4jGraph(\n",
        "        url=NEO4J_URI,\n",
        "        username=NEO4J_USER,\n",
        "        password=NEO4J_PASS,\n",
        "        # enhanced_schema=True tries to infer node labels/relationships automatically\n",
        "        enhanced_schema=True,\n",
        "    )\n",
        "    print(\"Detected schema:\\n\", graph.schema, \"\\n\")\n",
        "\n",
        "    # 3) Load local llama-cpp model\n",
        "    llm = LlamaCpp(\n",
        "        model_path=MODEL_PATH,\n",
        "        n_ctx=32768,\n",
        "        n_threads=8,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    # 4) Build the Cypher-aware QA chain\n",
        "    chain = GraphCypherQAChain.from_llm(\n",
        "        llm,\n",
        "        graph=graph,\n",
        "        cypher_prompt=CYTHER_ONLY_PROMPT,    # Our specialized prompt\n",
        "        validate_cypher=True,                # Validate the query\n",
        "        verbose=False,\n",
        "        allow_dangerous_requests=True,       # Allows MERGE if needed\n",
        "    )\n",
        "\n",
        "    # 5) Example questions for demonstration\n",
        "    questions = [\n",
        "        \"What are the top 5 most mentioned entities in these articles?\",\n",
        "    ]\n",
        "\n",
        "    # 6) Perform queries and display results\n",
        "    for q in questions:\n",
        "        print(f\"\\nQ: {q}\")\n",
        "        result = chain.invoke({\"query\": q})[\"result\"]  # key must be \"query\"\n",
        "        print(\"A:\", result)\n",
        "\n",
        "\n",
        "# run the RAG query\n",
        "main()\n",
        "print(\"\\n\\n\")\n",
        "print(\"RAG Complete!\")"
      ],
      "metadata": {
        "id": "cbrUrEllBS5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A couple of notes:\n",
        "\n",
        "* `Neo4jGraph` is a LangChain wrapper that uses the Neo4j Python driver under the hood. It can optionally introspect the database to understand the schema. If `enhanced_schema=True`, it will sample some data to list example property values which can help the LLM understand the domain (this can be useful, though not strictly required).\n",
        "* We printed `graph.schema` to see what it detected. It should list labels like **Document**, **Entity** and their properties (e.g., `Document` has properties `id`, `title`, `content`, etc.) and relationships like **MENTIONS**. This schema info will be provided to the LLM in its prompt context so that it knows how to form the Cypher queries.\n",
        "* We then load the LLM. We use `Llama` from LangChain, giving the path to our .gguf model file. We also specify `n_ctx=32768` to set a context window (adjust based on model capability). If you have GPU acceleration, you could pass parameters like `n_gpu_layers` or use a different BLAS, but CPU should work for a 7B model (albeit slowly). The `verbose=False` just suppresses internal logging from the LLM."
      ],
      "metadata": {
        "id": "GsM4XB4cBiCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding the LLM-Generated Cypher Queries"
      ],
      "metadata": {
        "id": "ZC-18AE-ufLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's worth noting that **we did not manually program any Cypher queries** for our Q&A - the LLM generated them on the fly based on the question and the graph schema. This demonstrates a powerful pattern:\n",
        "\n",
        "* The **knowledge graph** stores facts and relationships explicitly (documents, their topics, categories, etc.).\n",
        "* The **LLM** acts as a reasoner and translator, mapping a natural question to the right graph query, and then interpreting the results.\n",
        "\n",
        "LangChain's GraphCypherQAChain provided the scaffolding to make this happen easily. If you check the `verbose` output, you might see something like:\n",
        "\n",
        "```\n",
        "> Entering new GraphCypherQAChain chain...\n",
        "Generated Cypher:\n",
        "MATCH (d:Document)-[:MENTIONS]->(p:Entity)\n",
        "RETURN p.name, count(*) ORDER BY count(*) DESC LIMIT 5\n",
        "Full Context:\n",
        "[{'p.name': 'government', 'count(*)': 12}, {'p.name': 'Prime Minister', 'count(*)': 8}, ...]\n",
        "> Finished chain.\n",
        "```\n",
        "\n",
        "> **IMPORTANT:** It is also worth noting that this method is **EXTREMELY** brittle. Try changing the questions by rephrasing. How did it fair? Not good I am guessing."
      ],
      "metadata": {
        "id": "pZEeHgDrugdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Building a Graph-Based RAG Agent with Neo4j and Fixed Path Cyphers"
      ],
      "metadata": {
        "id": "_g_DyY4wujRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's dive into a powerful and efficient alternative approach: using predefined, or **fixed Cypher paths**, to interact with your Neo4j graph database within a Retrieval-Augmented Generation (RAG) agent. Rather than dynamically generating queries on-the-fly with an LLM, this method involves setting up specific, fixed Cypher queries optimized for common retrieval tasks. These carefully crafted queries ensure consistent performance, predictable results, and streamlined interactions with the database.\n",
        "\n",
        "In this section, you'll learn how to:\n",
        "\n",
        "* Define and utilize **fixed Cypher queries** designed for frequent and high-impact retrieval scenarios.\n",
        "* Connect these fixed paths directly into your RAG Agent workflow, enabling rapid responses to user questions without the overhead of dynamic query generation.\n",
        "\n",
        "By leveraging fixed Cypher paths, your RAG agent maintains efficiency, reduces complexity, and delivers reliable, lightning-fast answers tailored precisely to your application's needs."
      ],
      "metadata": {
        "id": "WXvi8Esqu0FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Using Precise/Fixed Cypher Paths Has MANY Benefits"
      ],
      "metadata": {
        "id": "tn0KpzS1vCzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will perform the RAG Query using our Graph-based implementation on Neo4j. Based on the data ingested, we will ask the RAG Agent the following questions:\n",
        "\n",
        "- What do these articles say about Ernie Wise?"
      ],
      "metadata": {
        "id": "13zGQN_5vFPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "RAG pipeline (Neo4j + spaCy + Llama-cpp)\n",
        "Model tested with TheBloke/neural-chat-7B-v3-3.Q4_K_M.gguf\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import spacy\n",
        "from neo4j import GraphDatabase\n",
        "from llama_cpp import Llama\n",
        "\n",
        "##############################################################################\n",
        "# Neo4j connection details\n",
        "##############################################################################\n",
        "\n",
        "NEO4J_URI      = os.getenv(\"NEO4J_URI\",      \"bolt://localhost:7687\")\n",
        "NEO4J_USER     = os.getenv(\"NEO4J_USER\",     \"neo4j\")\n",
        "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"neo4jneo4j\")\n",
        "\n",
        "##############################################################################\n",
        "# Llama-cpp configuration\n",
        "##############################################################################\n",
        "\n",
        "MODEL_PATH      = os.getenv(\"MODEL_PATH\", \"./neural-chat-7b-v3-3.Q4_K_M.gguf\")\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def load_llm() -> Llama:\n",
        "    \"\"\"\n",
        "    Load the GGUF model once, cache, and reuse.\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from {MODEL_PATH} …\")\n",
        "    return Llama(\n",
        "        model_path=MODEL_PATH,\n",
        "        n_ctx=32768,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        verbose=False,\n",
        "        chat_format=\"chatml\",  # Neural-Chat uses the ChatML template\n",
        "    )\n",
        "\n",
        "    # use_gpu=True,\n",
        "    # n_gpu_layers=-1,      # offload *all* transformer layers to the GPU\n",
        "    # n_threads=2,          # spawn enough CPU threads to feed the GPU\n",
        "    # n_batch=256,          # process 256 tokens at once for throughput\n",
        "    # f16_kv=True,          # store KV cache in half-precision on GPU\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# Neo4j helpers\n",
        "##############################################################################\n",
        "\n",
        "def connect_neo4j():\n",
        "    return GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "##############################################################################\n",
        "# spaCy Named-Entity Recognition\n",
        "##############################################################################\n",
        "\n",
        "def extract_entities_spacy(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text.strip(), ent.label_) for ent in doc.ents if len(ent.text.strip()) >= 3]\n",
        "\n",
        "##############################################################################\n",
        "# Graph query - fetch docs mentioning entities\n",
        "##############################################################################\n",
        "\n",
        "def fetch_documents_by_entities(session, entity_texts, top_k=5):\n",
        "    if not entity_texts:\n",
        "        return []\n",
        "\n",
        "    query = \"\"\"\n",
        "    MATCH (d:Document)-[:MENTIONS]->(e:Entity)\n",
        "    WHERE toLower(e.name) IN $entity_list\n",
        "    WITH d, count(e) as matchingEntities\n",
        "    ORDER BY matchingEntities DESC\n",
        "    LIMIT $topK\n",
        "    RETURN d.title AS title, d.content AS content,\n",
        "           d.category AS category, matchingEntities\n",
        "    \"\"\"\n",
        "    entity_list_lower = [txt.lower() for txt in entity_texts]\n",
        "\n",
        "    results = session.run(query,\n",
        "                          entity_list=entity_list_lower,\n",
        "                          topK=top_k)\n",
        "\n",
        "    docs = []\n",
        "    for r in results:\n",
        "        docs.append({\n",
        "            \"title\":  r[\"title\"],\n",
        "            \"content\": r[\"content\"],\n",
        "            \"category\": r[\"category\"],\n",
        "            \"match_count\": r[\"matchingEntities\"]\n",
        "        })\n",
        "    return docs\n",
        "\n",
        "##############################################################################\n",
        "# LLM-based answer generation\n",
        "##############################################################################\n",
        "\n",
        "def generate_answer(question: str, context: str) -> str:\n",
        "    llm = load_llm()\n",
        "\n",
        "    system_msg = \"You are an expert assistant answering questions using the given context.\"\n",
        "    user_prompt = (\n",
        "        f\"You are given the following context from multiple documents:\\n\"\n",
        "        f\"{context}\\n\\nQuestion: {question}\\n\\nProvide a concise answer.\"\n",
        "    )\n",
        "\n",
        "    response = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\",   \"content\": user_prompt},\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        max_tokens=32768,\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "##############################################################################\n",
        "# Main\n",
        "##############################################################################\n",
        "\n",
        "user_query = \"What do these articles say about Ernie Wise?\"\n",
        "print(f\"User Query: {user_query}\")\n",
        "\n",
        "# Load spaCy model once\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# NER over user query\n",
        "recognized_entities = extract_entities_spacy(user_query, nlp)\n",
        "entity_texts = [ent[0] for ent in recognized_entities]\n",
        "print(\"Recognized entities:\", recognized_entities)\n",
        "\n",
        "# Neo4j — fetch docs\n",
        "driver = connect_neo4j()\n",
        "with driver.session() as session:\n",
        "    docs = fetch_documents_by_entities(session, entity_texts, top_k=5)\n",
        "\n",
        "# Build context\n",
        "combined_context = \"\"\n",
        "for doc in docs:\n",
        "    snippet = doc[\"content\"][:300].replace(\"\\n\", \" \")\n",
        "    combined_context += (\n",
        "        f\"\\n---\\nTitle: {doc['title']} | Category: {doc['category']}\\n\"\n",
        "        f\"Snippet: {snippet}...\\n\"\n",
        "    )\n",
        "\n",
        "# Ask the model\n",
        "final_answer = generate_answer(user_query, combined_context)\n",
        "print(\"\\nRAG-based Answer:\\n\", final_answer)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"RAG Complete!\")"
      ],
      "metadata": {
        "id": "QO_1MXSUvELg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are four key considerations when using and extending this RAG pipeline:\n",
        "\n",
        "1. **Context Window & Token Limits**\n",
        "\n",
        "   * You must stay within the your LLMs context length (`n_ctx`) and the `max_tokens` you request. If your combined document snippets exceed this window, the model may truncate earlier context, reducing answer quality. The larger the context window, the better!\n",
        "   * Consider summarizing or chunking very long documents before passing them in.\n",
        "\n",
        "2. **Secure, Efficient Cypher Queries**\n",
        "\n",
        "   * Always use parameterized Cypher (as shown) to prevent injection attacks and leverage Neo4j's query planning.\n",
        "   * If you add an `expiration` filter on `[:MENTIONS]`, embed timestamps as parameters so Neo4j can cache and reuse the query plan.\n",
        "\n",
        "3. **NER Accuracy & Coverage**\n",
        "\n",
        "   * The small spaCy model (`en_core_web_sm`) is fast but may miss or mislabel domain-specific entities. For technical content, consider a larger or custom-trained NER model.\n",
        "   * Always normalize (e.g. `toLower()`) both your stored entity names and extracted entities to improve matching."
      ],
      "metadata": {
        "id": "eF87NvaC1U87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Understanding the Fixed Cypher Path Queries"
      ],
      "metadata": {
        "id": "tMSO4qpK1aNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to LLM-generated queries, **we explicitly define every Cypher statement** up front, mapping each question pattern to a predetermined graph traversal. This \"fixed path\" approach highlights a different but equally powerful pattern:\n",
        "\n",
        "* The **knowledge graph** still holds all facts and relationships explicitly (documents, entities, categories, etc.).\n",
        "* The **developer** now acts as the translator, hand-crafting precise Cypher templates that retrieve exactly the intended nodes and relationships.\n",
        "\n",
        "For example, consider a fixed query to fetch all documents mentioning a given entity:\n",
        "\n",
        "```cypher\n",
        "MATCH (d:Document)-[:MENTIONS]->(e:Entity {name: $entityName})\n",
        "RETURN\n",
        "  d.title      AS title,\n",
        "  d.category   AS category,\n",
        "  substring(d.content, 0, 200) AS snippet\n",
        "LIMIT $limit\n",
        "```\n",
        "\n",
        "When executed in Python:\n",
        "\n",
        "```python\n",
        "fixed_query = \"\"\"\n",
        "MATCH (d:Document)-[:MENTIONS]->(e:Entity {name: $entityName})\n",
        "RETURN d.title AS title, d.category AS category,\n",
        "       substring(d.content,0,200) AS snippet\n",
        "LIMIT $limit\n",
        "\"\"\"\n",
        "params = {\"entityName\": \"quantum computing\", \"limit\": 5}\n",
        "results = session.run(fixed_query, **params)\n",
        "for r in results:\n",
        "    print(f'Title: \"{r[\"title\"]}\" | Category: \"{r[\"category\"]}\"\\nSnippet: {r[\"snippet\"]}…')\n",
        "```\n",
        "\n",
        "You might see output such as:\n",
        "\n",
        "```\n",
        "Title: \"Quantum Leap in Computing\"   | Category: \"tech\"\n",
        "Snippet: \"Researchers at Google have unveiled a new 72-qubit quantum processor…\"…\n",
        "\n",
        "Title: \"Advances in Quantum Hardware\" | Category: \"science\"\n",
        "Snippet: \"A team at IBM demonstrated error correction on a superconducting quantum…\"…\n",
        "```\n",
        "\n",
        "This fixed-path method offers clear benefits:\n",
        "\n",
        "* **Predictable Performance**: Neo4j can cache and optimize these well-known queries.\n",
        "* **Deterministic Results**: Each question corresponds to a single, auditable Cypher template.\n",
        "* **Easier Maintenance & Security**: Queries are defined in code, making it simpler to review, test, and secure against injection.\n",
        "\n",
        "By combining fixed Cypher paths with RAG-style context assembly (or even alongside LLM-generated queries), you gain full control over both performance and flexibility in your graph-powered Q&A system."
      ],
      "metadata": {
        "id": "2ZJ8GGfv1btL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop End!"
      ],
      "metadata": {
        "id": "_BF-oZad5pGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This covers using both LLM-generated Cypher Paths using GraphCypherQAChain and Fixed Cypher Paths. Let's move onto the next section to understand how Reinforcement Learning works in a Graph-based RAG implementations!"
      ],
      "metadata": {
        "id": "k_JC58If5rym"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
